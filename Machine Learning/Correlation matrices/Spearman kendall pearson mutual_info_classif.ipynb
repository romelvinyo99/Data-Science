{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba5db288-abcc-4861-94b9-6164c57941f1",
   "metadata": {},
   "source": [
    "Types of correlation matrixes\n",
    "1.pearson  - linear relationships\n",
    "2.kendall  - ordinal associations between variables\n",
    "3.spearman - monotonic relationships(which can be non - linear)\n",
    "-->There is another technique - checks for mutual information that can capture any non-linear relationships\n",
    "X = df.drop(columns = [\"Churn\"], axis = 1)\n",
    "y = df[\"Churn\"]\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mutual_info = mutual_info_classif(X, y)\n",
    "mutual_info_series = pd.Series(mutual_info, index = X.columns)\n",
    "mutual_info_series.sort_values(ascending = False)\n",
    "Choosing the appropriate model based on the type of correlation (linear, monotonic, non-linear) can significantly impact the performance of your predictive model. Here's a guide to help you decide which types of models to consider based on the correlations you've observed:\n",
    "\n",
    "Choosing the correct model\n",
    "\n",
    "1. Linear Relationship (Pearson Correlation)\n",
    "Indicators: Strong Pearson correlation (close to 1 or -1).\n",
    "Models to Consider:\n",
    "Linear Regression: Good for predicting continuous outcomes when the relationship between features and target is linear.\n",
    "Logistic Regression: Suitable for binary classification tasks with linear decision boundaries.\n",
    "Ridge/Lasso Regression: Variants of linear regression that include regularization, useful when you have many correlated features.\n",
    "2. Monotonic Relationship (Spearman/Kendall Correlation)\n",
    "Indicators: Strong Spearman or Kendall correlation.\n",
    "Models to Consider:\n",
    "Decision Trees: Can capture both linear and non-linear relationships, and they don't assume linearity in the data.\n",
    "Random Forests: An ensemble of decision trees, robust to overfitting, and captures complex relationships.\n",
    "Gradient Boosting Machines (GBM): Builds models sequentially and can capture monotonic and non-linear relationships effectively.\n",
    "3. Non-Linear Relationship\n",
    "Indicators: Low Pearson correlation but noticeable trends in scatter plots or high mutual information scores.\n",
    "Models to Consider:\n",
    "Support Vector Machines (SVM): Especially with non-linear kernels (like RBF), SVMs can capture complex boundaries.\n",
    "Neural Networks: Good for capturing highly non-linear relationships, especially with multiple hidden layers (Deep Learning).\n",
    "Polynomial Regression: Extends linear regression to capture non-linear relationships by adding polynomial terms of the features.\n",
    "4. No Clear Relationship\n",
    "Indicators: All correlations (Pearson, Spearman, Kendall) are weak or close to 0.\n",
    "Models to Consider:\n",
    "Ensemble Methods: Random Forests, Gradient Boosting, or even stacking models can combine multiple weak learners to improve performance.\n",
    "Clustering Algorithms: If relationships are not clear, unsupervised methods like K-Means or DBSCAN might help identify hidden patterns in the data.\n",
    "Dimensionality Reduction: Techniques like PCA (Principal Component Analysis) might be necessary to uncover latent structures in the data.\n",
    "Practical Approach\n",
    "Explore the Data: Start by visualizing the data (e.g., scatter plots, heatmaps) to understand the relationships.\n",
    "Run Multiple Models: In practice, it's often useful to try several models (linear, tree-based, etc.) and evaluate their performance using cross-validation.\n",
    "Feature Engineering: Consider transforming your features (e.g., polynomial features, logarithmic transformations) based on observed relationships to improve model performance.\n",
    "Example Workflow\n",
    "Check Correlations:\n",
    "\n",
    "Use df.corr() with different methods to understand relationships.\n",
    "Visualize data using scatter plots or pair plots.\n",
    "Select Models:\n",
    "\n",
    "If strong linear correlations are observed, start with linear models.\n",
    "If non-linear patterns are detected, consider tree-based methods or neural networks.\n",
    "Evaluate:\n",
    "\n",
    "Use metrics like RÂ², RMSE (for regression), or accuracy, AUC (for classification) to compare models.\n",
    "Perform cross-validation to ensure the model generalizes well.\n",
    "Iterate:\n",
    "\n",
    "Refine your model selection based on performance. Consider hyperparameter tuning, feature selection, or model ensembles.\n",
    "By combining correlation analysis with a well-rounded exploration of models, you can select the most appropriate model for your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
